<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Attention-based Neural Machine Translation with Keras</title>
  
  <meta property="description" itemprop="description" content="As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2018-07-30"/>
  <meta property="article:created" itemprop="dateCreated" content="2018-07-30"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Attention-based Neural Machine Translation with Keras"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Attention-based Neural Machine Translation with Keras"/>
  <meta property="twitter:description" content="As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Effective approaches to attention-based neural machine translation;citation_publication_date=2015;citation_volume=abs/1508.04025;citation_author=Minh-Thang Luong;citation_author=Hieu Pham;citation_author=Christopher D. Manning"/>
  <meta name="citation_reference" content="citation_title=Grammar as a foreign language;citation_publication_date=2014;citation_volume=abs/1412.7449;citation_author=Oriol Vinyals;citation_author=Lukasz Kaiser;citation_author=Terry Koo;citation_author=Slav Petrov;citation_author=Ilya Sutskever;citation_author=Geoffrey E. Hinton"/>
  <meta name="citation_reference" content="citation_title=Show, attend and tell: Neural image caption generation with visual attention;citation_publication_date=2015;citation_volume=abs/1502.03044;citation_author=Kelvin Xu;citation_author=Jimmy Ba;citation_author=Ryan Kiros;citation_author=Kyunghyun Cho;citation_author=Aaron C. Courville;citation_author=Ruslan Salakhutdinov;citation_author=Richard S. Zemel;citation_author=Yoshua Bengio"/>
  <meta name="citation_reference" content="citation_title=Neural machine translation by jointly learning to align and translate;citation_publication_date=2014;citation_volume=abs/1409.0473;citation_author=Dzmitry Bahdanau;citation_author=Kyunghyun Cho;citation_author=Yoshua Bengio"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","bibliography","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Attention-based Neural Machine Translation with Keras"]},{"type":"character","attributes":{},"value":["As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["http://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["07-30-2018"]},{"type":"character","attributes":{},"value":["Attention","Translation","Keras","Eager"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/attention.png"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["attention-layer_files/bowser-1.9.3/bowser.min.js","attention-layer_files/distill-2.2.21/template.v2.js","attention-layer_files/jquery-1.11.3/jquery.min.js","attention-layer_files/webcomponents-2.0.0/webcomponents.js","bibliography.bib","images/attention.png","images/train-1.png","images/train-4-thumbnail.png","images/train-4.png","images/validation-3.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.text();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.text(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        if ($.inArray(language, ["r", "cpp", "c", "java"]) != -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      $(this).find('img, .html-widget').css('width', '100%');
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="attention-layer_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="attention-layer_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="attention-layer_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="attention-layer_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Attention-based Neural Machine Translation with Keras","description":"As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"http://www.rstudio.com/"}],"publishedDate":"2018-07-30T00:00:00.000+02:00","citationText":"Keydana, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Attention-based Neural Machine Translation with Keras</h1>
<p>As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.</p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="http://www.rstudio.com/" class="uri">http://www.rstudio.com/</a>
  
<br/>07-30-2018
</div>

<div class="d-article">
<p>These days it is not difficult to find sample code that demonstrates sequence to sequence translation using Keras. However, within the past few years it has been established that depending on the task, incorporating an attention mechanism significantly improves performance. First and foremost, this was the case for neural machine translation (see <span class="citation" data-cites="BahdanauCB14">(Bahdanau, Cho, and Bengio <a href="#ref-BahdanauCB14">2014</a>)</span> and <span class="citation" data-cites="LuongPM15">(Luong, Pham, and Manning <a href="#ref-LuongPM15">2015</a>)</span> for prominent work). But other areas performing sequence to sequence translation were profiting from incorporating an attention mechanism, too: E.g., <span class="citation" data-cites="XuBKCCSZB15">(Xu et al. <a href="#ref-XuBKCCSZB15">2015</a>)</span> applied attention to image captioning, and <span class="citation" data-cites="VinyalsKKPSH14">(Vinyals et al. <a href="#ref-VinyalsKKPSH14">2014</a>)</span>, to parsing.</p>
<p>Ideally, using Keras, we’d just have an attention layer managing this for us. Unfortunately, as can be seen googling for code snippets and blog posts, implementing attention in pure Keras is not that straightforward.</p>
<p>Consequently, until a short time ago, the best thing to do seemed to be translating the <a href="https://github.com/tensorflow/nmt">TensorFlow Neural Machine Translation Tutorial</a> to R TensorFlow. Then, <a href="https://www.tensorflow.org/guide/eager">TensorFlow eager execution</a> happened, and turned out a game changer for a number of things that used to be difficult (not the least of which is debugging). With eager execution, tensor operations are executed immediately, as opposed to of building a graph to be evaluated later. This means we can immediately inspect the values in our tensors - and it also means we can imperatively code loops to perform interleavings of sorts that earlier were more challenging to accomplish.</p>
<p>Under these circumstances, it is not surprising that the <a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb">interactive notebook on neural machine translation</a>, published on Colaboratory, got a lot of attention for its straightforward implementation and highly intellegible explanations. Our goal here is to do the same thing from R. We will not end up with Keras code exactly the way we used to write it, but a hybrid of Keras layers and imperative code enabled by TensorFlow eager execution.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>The code in this post depends on the development versions of several of the TensorFlow R packages. You can install these packages as follows:</p>
<pre class="r"><code>
devtools::install_github(c(
  &quot;rstudio/reticulate&quot;,
  &quot;rstudio/tensorflow&quot;,
  &quot;rstudio/keras&quot;,
  &quot;rstudio/tfdatasets&quot;
))</code></pre>
<p>You should also be sure that you are running the very latest version of TensorFlow (v1.9), which you can install like so:</p>
<pre class="r"><code>
library(tensorflow)
install_tensorflow()</code></pre>
<p>There are additional requirements for using TensorFlow eager execution. First, we need to call <code>tfe_enable_eager_execution()</code> right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation. This is because at a later point, we are going to access <code>model$variables</code> which at this point does not exist in base Keras.</p>
<p>We’ll also use the <a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html">tfdatasets</a> package for our input pipeline. So we end up with the following libraries needed for this example:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(keras)
use_implementation(&quot;tensorflow&quot;)

library(tensorflow)
tfe_enable_eager_execution()

library(tfdatasets)

library(purrr)
library(stringr)
library(reshape2)
library(viridis)
library(ggplot2)
library(tibble)</code></pre>
</div>
<h2 id="preparing-the-data">Preparing the data</h2>
<p>As our focus is on implementing the attention mechanism, we’re going to do a quick pass through pre-preprocessing. All operations are contained in short functions that are independently testable (which also makes it easy should you want to experiment with different preprocessing actions).</p>
<p>The site <a href="https://www.manythings.org/anki/" class="uri">https://www.manythings.org/anki/</a> is a great source for multilingual datasets. For variation, we’ll choose a different dataset from the colab notebook, and try to translate English to Dutch. I’m going to assume you have the unzipped file <code>nld.txt</code> in a subdirectory called <code>data</code> in your current directory. The file contains 28224 sentence pairs, of which we are going to use the first 10000. Under this restriction, sentences range from one-word exclamations</p>
<pre><code>
Run!    Ren!
Wow!    Da&#39;s niet gek!
Fire!   Vuur!</code></pre>
<p>over short phrases</p>
<pre><code>
Are you crazy?  Ben je gek?
Do cats dream?  Dromen katten?
Feed the bird!  Geef de vogel voer!</code></pre>
<p>to simple sentences such as</p>
<pre><code>
My brother will kill me.    Mijn broer zal me vermoorden.
No one knows the future.    Niemand kent de toekomst.
Please ask someone else.    Vraag alsjeblieft iemand anders.</code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
filepath &lt;- file.path(&quot;data&quot;, &quot;nld.txt&quot;)

lines &lt;- readLines(filepath, n = 10000)
sentences &lt;- str_split(lines, &quot;\t&quot;)</code></pre>
</div>
<p>Basic preprocessing includes adding space before punctuation, replacing special characters, reducing multiple spaces to one, and adding <code>&lt;start&gt;</code> and <code>&lt;stop&gt;</code> tokens at the beginnings resp. ends of the sentences.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
space_before_punct &lt;- function(sentence) {
  str_replace_all(sentence, &quot;([?.!])&quot;, &quot; \\1&quot;)
}

replace_special_chars &lt;- function(sentence) {
  str_replace_all(sentence, &quot;[^a-zA-Z?.!,¿]+&quot;, &quot; &quot;)
}

add_tokens &lt;- function(sentence) {
  paste0(&quot;&lt;start&gt; &quot;, sentence, &quot; &lt;stop&gt;&quot;)
}
add_tokens &lt;- Vectorize(add_tokens, USE.NAMES = FALSE)

preprocess_sentence &lt;- compose(add_tokens,
                               str_squish,
                               replace_special_chars,
                               space_before_punct)

word_pairs &lt;- map(sentences, preprocess_sentence)</code></pre>
</div>
<p>As usual with text data, we need to create lookup indices to get from words to integers and vice versa: one index each for the source and target languages.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
create_index &lt;- function(sentences) {
  unique_words &lt;- sentences %&gt;% unlist() %&gt;% paste(collapse = &quot; &quot;) %&gt;%
    str_split(pattern = &quot; &quot;) %&gt;% .[[1]] %&gt;% unique() %&gt;% sort()
  index &lt;- data.frame(
    word = unique_words,
    index = 1:length(unique_words),
    stringsAsFactors = FALSE
  ) %&gt;%
    add_row(word = &quot;&lt;pad&gt;&quot;,
                    index = 0,
                    .before = 1)
  index
}

word2index &lt;- function(word, index_df) {
  index_df[index_df$word == word, &quot;index&quot;]
}
index2word &lt;- function(index, index_df) {
  index_df[index_df$index == index, &quot;word&quot;]
}

src_index &lt;- create_index(map(word_pairs, ~ .[[1]]))
target_index &lt;- create_index(map(word_pairs, ~ .[[2]]))</code></pre>
</div>
<p>Conversion of text to integers uses the above indices as well as Keras’ convenient <code>pad_sequences</code> function, which leaves us with matrices of integers, padded up to maximum sentence length found in the source and target corpora, respectively.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
sentence2digits &lt;- function(sentence, index_df) {
  map((sentence %&gt;% str_split(pattern = &quot; &quot;))[[1]], function(word)
    word2index(word, index_df))
}

sentlist2diglist &lt;- function(sentence_list, index_df) {
  map(sentence_list, function(sentence)
    sentence2digits(sentence, index_df))
}

src_diglist &lt;-
  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)
src_maxlen &lt;- map(src_diglist, length) %&gt;% unlist() %&gt;% max()
src_matrix &lt;-
  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = &quot;post&quot;)

target_diglist &lt;-
  sentlist2diglist(map(word_pairs, ~ .[[2]]), target_index)
target_maxlen &lt;- map(target_diglist, length) %&gt;% unlist() %&gt;% max()
target_matrix &lt;-
  pad_sequences(target_diglist, maxlen = target_maxlen, padding = &quot;post&quot;)</code></pre>
</div>
<p>All that remains to be done is the train-test split.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
train_indices &lt;-
  sample(nrow(src_matrix), size = nrow(src_matrix) * 0.8)

validation_indices &lt;- setdiff(1:nrow(src_matrix), train_indices)

x_train &lt;- src_matrix[train_indices, ]
y_train &lt;- target_matrix[train_indices, ]

x_valid &lt;- src_matrix[validation_indices, ]
y_valid &lt;- target_matrix[validation_indices, ]

buffer_size &lt;- nrow(x_train)

# just for convenience, so we may get a glimpse at translation 
# performance during training
train_sentences &lt;- sentences[train_indices]
validation_sentences &lt;- sentences[validation_indices]
validation_sample &lt;- sample(validation_sentences, 5)</code></pre>
</div>
<h2 id="creating-datasets-to-iterate-over">Creating datasets to iterate over</h2>
<p>This section does not contain much code, but it shows an important technique: the use of datasets. Remember the olden times when we used to pass in hand-crafted generators to Keras models? With <a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html">tfdatasets</a>, we can scalably feed data directly to the Keras <code>fit</code> function, having various preparatory actions being performed directly in native code. In our case, we will not be using <code>fit</code>, instead iterate directly over the tensors contained in the dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
train_dataset &lt;- 
  tensor_slices_dataset(keras_array(list(x_train, y_train)))  %&gt;%
  dataset_shuffle(buffer_size = buffer_size) %&gt;%
  dataset_batch(batch_size, drop_remainder = TRUE)

validation_dataset &lt;-
  tensor_slices_dataset(keras_array(list(x_valid, y_valid))) %&gt;%
  dataset_shuffle(buffer_size = buffer_size) %&gt;%
  dataset_batch(batch_size, drop_remainder = TRUE)</code></pre>
</div>
<p>Now we are ready to roll! In fact, before talking about that training loop we need to dive into the implementation of the core logic: the custom layers responsible for performing the attention operation.</p>
<h2 id="attention-encoder">Attention encoder</h2>
<p>We will create two custom layers, only the second of which is going to incorporate attention logic.</p>
<p>However, it’s worth introducing the encoder in detail too, because technically this is not a custom layer but a custom model, as described <a href="https://github.com/rstudio/keras/blob/master/vignettes/custom_models.Rmd">here</a>.</p>
<p>Custom models allow you to create member layers and then, specify custom functionality defining the operations to be performed on these layers.</p>
<p>Let’s look at the complete code for the encoder.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
attention_encoder &lt;-
  
  function(gru_units,
           embedding_dim,
           src_vocab_size,
           name = NULL) {
    
    keras_model_custom(name = name, function(self) {
      
      self$embedding &lt;-
        layer_embedding(
          input_dim = src_vocab_size,
          output_dim = embedding_dim
        )
      
      self$gru &lt;-
        layer_gru(
          units = gru_units,
          return_sequences = TRUE,
          return_state = TRUE
        )
      
      function(inputs, mask = NULL) {
        
        x &lt;- inputs[[1]]
        hidden &lt;- inputs[[2]]
        
        x &lt;- self$embedding(x)
        c(output, state) %&lt;-% self$gru(x, initial_state = hidden)
    
        list(output, state)
      }
    })
  }</code></pre>
</div>
<p>The encoder has two layers, an embedding and a GRU layer. The ensuing anonymous function specifies what should happen when the layer is called. One thing that might look unexpected is the argument passed to that function: It is a list of tensors, where the first element are the inputs, and the second is the hidden state at the point the layer is called (in traditional Keras RNN usage, we are accustomed to seeing state manipulations being done transparently for us.) As the input to the call flows through the operations, let’s keep track of the shapes involved:</p>
<ul>
<li><p><code>x</code>, the input, is of size <code>(batch_size, max_length_input)</code>, where <code>max_length_input</code> is the number of digits constituting a source sentence. (Remember we’ve padded them to be of uniform length.) In familiar RNN parlance, we could also speak of <code>timesteps</code> here (we soon will).</p></li>
<li><p>After the embedding step, the tensors will have an additional axis, as each timestep (token) will have been embedded as an <code>embedding_dim</code>-dimensional vector. So our shapes are now <code>(batch_size, max_length_input, embedding_dim)</code>.</p></li>
<li><p>Note how when calling the GRU, we’re passing in the hidden state we received as <code>initial_state</code>. We get back a list: the GRU output and last hidden state.</p></li>
</ul>
<p>At this point, it helps to look up RNN output shapes in the <a href="https://tensorflow.rstudio.com/keras/reference/layer_simple_rnn.html">documentation</a>.</p>
<p>We have specified our GRU to return sequences as well as the state. Our asking for the state means we’ll get back a list of tensors: the output, and the last state(s) - a single last state in this case as we’re using GRU. That state itself will be of shape <code>(batch_size, gru_units)</code>. Our asking for sequences means the output will be of shape <code>(batch_size, max_length_input, gru_units)</code>. So that’s that. We bundle output and last state in a list and pass it to the calling code.</p>
<p>Before we show the decoder, we need to say a few things about attention.</p>
<h2 id="attention-in-a-nutshell">Attention in a nutshell</h2>
<p>As T. Luong nicely puts it in his <a href="https://github.com/lmthang/thesis/blob/master/thesis.pdf">thesis</a>, the idea of the attention mechanism is</p>
<blockquote>
<p>to provide a ‘random access memory’ of source hidden states which one can constantly refer to as translation progresses.</p>
</blockquote>
<p>This means that at every timestep, the decoder receives not just the previous decoder hidden state, but also the complete output from the encoder. It then “makes up its mind” as to what part of the encoded input matters at the current point in time. Although various attention mechanisms exist, the basic procedure often goes like this.</p>
<aside>
In our description, we’re closely following <span class="citation" data-cites="LuongPM15">(Luong, Pham, and Manning <a href="#ref-LuongPM15">2015</a>)</span>, in accordance with the colaboratory notebook on NMT.
</aside>
<p>First, we create a <em>score</em> that relates the decoder hidden state at a given timestep to the encoder hidden states at every timestep.</p>
<p>The score function can take different shapes; the following is commonly referred to as <em>Bahdanau style</em> (additive) attention.</p>
<p>Note that when referring to this as <em>Bahdanau style attention</em>, we - like others - do not imply exact agreement with the formulae in <span class="citation" data-cites="BahdanauCB14">(Bahdanau, Cho, and Bengio <a href="#ref-BahdanauCB14">2014</a>)</span>. It is about the general way encoder and decoder hidden states are combined - additively or multiplicatively.</p>
<p><span class="math display">\[score(\mathbf{h}_t,\bar{\mathbf{h}_s}) = \mathbf{v}_a^T tanh(\mathbf{W_1}\mathbf{h}_t + \mathbf{W_2}\bar{\mathbf{h}_s})\]</span></p>
<p>From these scores, we want to find the encoder states that matter most to the current decoder timestep. Basically, we just normalize the scores doing a softmax, which leaves us with a set of <em>attention weights</em> (also called <em>alignment vectors</em>):</p>
<p><span class="math display">\[\alpha_{ts} = \frac{exp(score(\mathbf{h}_t,\bar{\mathbf{h}_s}))}{\sum_{s&#39;=1}^{S}{score(\mathbf{h}_t,\bar{\mathbf{h}_{s&#39;}})}}\]</span></p>
<p>From these <em>attention weights</em>, we create the <em>context vector</em>. This is basically an average of the source hidden states, weighted by the <em>attention weights</em>:</p>
<p><span class="math display">\[\mathbf{c}_t= \sum_s{\alpha_{ts} \bar{\mathbf{h}_s}}\]</span></p>
<p>Now we need to relate this to the state the decoder is in. We calculate the <em>attention vector</em> from a concatenation of context vector and current decoder hidden state:</p>
<p><span class="math display">\[\mathbf{a}_t = tanh(\mathbf{W_c} [ \mathbf{c}_t ; \mathbf{h}_t])\]</span></p>
<p>In sum, we see how at each timestep, the attention mechanism combines information from the sequence of encoder states, and the current decoder hidden state. We’ll soon see a third source of information entering the calculation, which will be dependent on whether we’re in the training or the prediction phase.</p>
<h2 id="attention-decoder">Attention decoder</h2>
<p>Now let’s look at how the attention decoder implements the above logic. We will be following the colab notebook in presenting a slight simplification of the score function, which will not prevent the decoder from successfully translating our example sentences.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
attention_decoder &lt;-
  function(object,
           gru_units,
           embedding_dim,
           target_vocab_size,
           name = NULL) {
    
    keras_model_custom(name = name, function(self) {
      
      self$gru &lt;-
        layer_gru(
          units = gru_units,
          return_sequences = TRUE,
          return_state = TRUE
        )
      
      self$embedding &lt;-
        layer_embedding(input_dim = target_vocab_size, 
                        output_dim = embedding_dim)
      
      gru_units &lt;- gru_units
      self$fc &lt;- layer_dense(units = target_vocab_size)
      self$W1 &lt;- layer_dense(units = gru_units)
      self$W2 &lt;- layer_dense(units = gru_units)
      self$V &lt;- layer_dense(units = 1L)
 
      function(inputs, mask = NULL) {
        
        x &lt;- inputs[[1]]
        hidden &lt;- inputs[[2]]
        encoder_output &lt;- inputs[[3]]
        
        hidden_with_time_axis &lt;- k_expand_dims(hidden, 2)
        
        score &lt;- self$V(k_tanh(self$W1(encoder_output) + 
                                self$W2(hidden_with_time_axis)))
        
        attention_weights &lt;- k_softmax(score, axis = 2)
        
        context_vector &lt;- attention_weights * encoder_output
        context_vector &lt;- k_sum(context_vector, axis = 2)
    
        x &lt;- self$embedding(x)
       
        x &lt;- k_concatenate(list(k_expand_dims(context_vector, 2), x), axis = 3)
        
        c(output, state) %&lt;-% self$gru(x)
   
        output &lt;- k_reshape(output, c(-1, gru_units))
    
        x &lt;- self$fc(output)
 
        list(x, state, attention_weights)
        
      }
      
    })
  }</code></pre>
</div>
<p>Firstly, we notice that in addition to the usual embedding and GRU layers we’d expect in a decoder, there are a few additional dense layers. We’ll comment on those as we go.</p>
<p>This time, the first argument to what is effectively the <code>call</code> function consists of three parts: input, hidden state, and the output from the encoder.</p>
<p>First we need to calculate the score, which basically means addition of two matrix multiplications. For that addition, the shapes have to match. Now <code>encoder_output</code> is of shape <code>(batch_size, max_length_input, gru_units)</code>, while <code>hidden</code> has shape <code>(batch_size, gru_units)</code>. We thus add an axis “in the middle”, obtaining <code>hidden_with_time_axis</code>, of shape <code>(batch_size, 1, gru_units)</code>.</p>
<p>After applying the <code>tanh</code> and the fully connected layer to the result of the addition, <code>score</code> will be of shape <code>(batch_size, max_length_input, 1)</code>. The next step calculates the softmax, to get the <em>attention weights</em>. Now softmax by default is applied on the last axis - but here we’re applying it on the second axis, since it is with respect to the input timesteps we want to normalize the scores.</p>
<p>After normalization, the shape is still <code>(batch_size, max_length_input, 1)</code>.</p>
<p>Next up we compute the context vector, as a weighted average of encoder hidden states. Its shape is <code>(batch_size, gru_units)</code>. Note that like with the softmax operation above, we sum over the second axis, which corresponds to the number of timesteps in the input received from the encoder.</p>
<p>We still have to take care of the third source of information: the input. Having been passed through the embedding layer, its shape is <code>(batch_size, 1, embedding_dim)</code>. Here, the second axis is of dimension 1 as we’re forecasting a single token at a time.</p>
<p>Now, let’s concatenate the context vector and the embedded input, to arrive at the <em>attention vector</em>. If you compare the code with the formula above, you’ll see that here we’re skipping the <code>tanh</code> and the additional fully connected layer, and just leave it at the concatenation. After concatenation, the shape now is <code>(batch_size, 1, embedding_dim + gru_units)</code>.</p>
<p>The ensuing GRU operation, as usual, gives us back output and shape tensors. The output tensor is flattened to shape <code>(batch_size, gru_units)</code> and passed through the final densely connected layer, after which the output has shape <code>(batch_size, target_vocab_size)</code>. With that, we’re going to be able to forecast the next token for every input in the batch.</p>
<p>Remains to return everything we’re interested in: the output (to be used for forecasting), the last GRU hidden state (to be passed back in to the decoder), and the <em>attention weights</em> for this batch (for plotting). And that’s that!</p>
<h2 id="creating-the-model">Creating the “model”</h2>
<p>We’re almost ready to train the model. The model? We don’t have a model yet. The next steps will feel a bit unusual if you’re accustomed to the traditional Keras <em>create model -&gt; compile model -&gt; fit model </em> workflow. Let’s have a look.</p>
<p>First, we need a few bookkeeping variables.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
batch_size &lt;- 32
embedding_dim &lt;- 64
gru_units &lt;- 256

src_vocab_size &lt;- nrow(src_index)
target_vocab_size &lt;- nrow(target_index)</code></pre>
</div>
<p>Now, we create the encoder and decoder objects - it’s tempting to call them layers, but technically both are custom Keras models.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
encoder &lt;- attention_encoder(
  gru_units = gru_units,
  embedding_dim = embedding_dim,
  src_vocab_size = src_vocab_size
)

decoder &lt;- attention_decoder(
  gru_units = gru_units,
  embedding_dim = embedding_dim,
  target_vocab_size = target_vocab_size
)</code></pre>
</div>
<p>So as we’re going along, assembling a model “from pieces”, we still need a loss function, and an optimizer.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
optimizer &lt;- tf$train$AdamOptimizer()

cx_loss &lt;- function(y_true, y_pred) {
  mask &lt;- ifelse(y_true == 0L, 0, 1)
  loss &lt;-
    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true,
                                                   logits = y_pred) * mask
  tf$reduce_mean(loss)
}</code></pre>
</div>
<p>Now we’re ready to train.</p>
<h2 id="training-phase">Training phase</h2>
<p>In the training phase, we’re using <em>teacher forcing</em>, which is the established name for feeding the model the (correct) target at time <span class="math inline">\(t\)</span> as input for the next calculation step at time <span class="math inline">\(t + 1\)</span>. This is in contrast to the inference phase, when the decoder output is fed back as input to the next time step.</p>
<p>The training phase consists of three loops: firstly, we’re looping over epochs, secondly, over the dataset, and thirdly, over the target sequence we’re predicting.</p>
<p>For each batch, we’re encoding the source sequence, getting back the output sequence as well as the last hidden state. The hidden state we then use to initialize the decoder. Now, we enter the target sequence prediction loop. For each timestep to be predicted, we call the decoder with the input (which due to teacher forcing is the ground truth from the previous step), its previous hidden state, and the complete encoder output. At each step, the decoder returns predictions, its hidden state and the attention weights.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
n_epochs &lt;- 50

encoder_init_hidden &lt;- k_zeros(c(batch_size, gru_units))

for (epoch in seq_len(n_epochs)) {
  
  total_loss &lt;- 0
  iteration &lt;- 0
    
  iter &lt;- make_iterator_one_shot(train_dataset)
    
  until_out_of_range({
    
    batch &lt;- iterator_get_next(iter)
    loss &lt;- 0
    x &lt;- batch[[1]]
    y &lt;- batch[[2]]
    iteration &lt;- iteration + 1
      
    with(tf$GradientTape() %as% tape, {
      c(enc_output, enc_hidden) %&lt;-% encoder(list(x, encoder_init_hidden))
 
      dec_hidden &lt;- enc_hidden
      dec_input &lt;-
        k_expand_dims(rep(list(
          word2index(&quot;&lt;start&gt;&quot;, target_index)
        ), batch_size))
        

      for (t in seq_len(target_maxlen - 1)) {
        c(preds, dec_hidden, weights) %&lt;-%
          decoder(list(dec_input, dec_hidden, enc_output))
        loss &lt;- loss + cx_loss(y[, t], preds)
     
        dec_input &lt;- k_expand_dims(y[, t])
      }
      
    })
      
    total_loss &lt;-
      total_loss + loss / k_cast_to_floatx(dim(y)[2])
      
      paste0(
        &quot;Batch loss (epoch/batch): &quot;,
        epoch,
        &quot;/&quot;,
        iter,
        &quot;: &quot;,
        (loss / k_cast_to_floatx(dim(y)[2])) %&gt;% 
          as.double() %&gt;% round(4),
        &quot;\n&quot;
      )
      
    variables &lt;- c(encoder$variables, decoder$variables)
    gradients &lt;- tape$gradient(loss, variables)
      
    optimizer$apply_gradients(
      purrr::transpose(list(gradients, variables)),
      global_step = tf$train$get_or_create_global_step()
    )
      
  })
    
    paste0(
      &quot;Total loss (epoch): &quot;,
      epoch,
      &quot;: &quot;,
      (total_loss / k_cast_to_floatx(buffer_size)) %&gt;% 
        as.double() %&gt;% round(4),
      &quot;\n&quot;
    )
}</code></pre>
</div>
<p>How does backpropagation work with this new flow? With eager execution, a <code>GradientTape</code> records operations performed on the forward pass. This recording is then “played back” to perform backpropagation. Concretely put, during the forward pass, we have the tape recording the model’s actions, and we keep incrementally updating the loss. Then, outside the tape’s context, we ask the tape for the gradients of the accumulated loss with respect to the model’s variables. Once we know the gradients, we can have the optimizer apply them to those variables. This <code>variables</code> slot, by the way, does not (as of this writing) exist in the base implementation of Keras, which is why we have to resort to the TensorFlow implementation.</p>
<h2 id="inference">Inference</h2>
<p>As soon as we have a trained model, we can get translating! Actually, we don’t have to wait. We can integrate a few sample translations directly into the training loop, and watch the network progressing (hopefully!). The <a href="https://github.com/rstudio/keras/tree/master/vignettes/examples/nmt_attention.R">complete code for this post</a> will do it like this, however here we’re arranging the steps in a more didactical order.</p>
<p>The inference loop differs from the training procedure mainly it that it does not use teacher forcing. Instead, we feed back the current prediction as input to the next decoding timestep. The actual predicted word is chosen from the exponentiated raw scores returned by the decoder using a multinomial distribution. We also include a function to plot a heatmap that shows where in the source attention is being directed as the translation is produced.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
evaluate &lt;-
  function(sentence) {
    attention_matrix &lt;-
      matrix(0, nrow = target_maxlen, ncol = src_maxlen)
    
    sentence &lt;- preprocess_sentence(sentence)
    input &lt;- sentence2digits(sentence, src_index)
    input &lt;-
      pad_sequences(list(input), maxlen = src_maxlen,  padding = &quot;post&quot;)
    input &lt;- k_constant(input)
    
    result &lt;- &quot;&quot;
    
    hidden &lt;- k_zeros(c(1, gru_units))
    c(enc_output, enc_hidden) %&lt;-% encoder(list(input, hidden))
    
    dec_hidden &lt;- enc_hidden
    dec_input &lt;-
      k_expand_dims(list(word2index(&quot;&lt;start&gt;&quot;, target_index)))
    
    for (t in seq_len(target_maxlen - 1)) {
      c(preds, dec_hidden, attention_weights) %&lt;-%
        decoder(list(dec_input, dec_hidden, enc_output))
      attention_weights &lt;- k_reshape(attention_weights, c(-1))
      attention_matrix[t, ] &lt;- attention_weights %&gt;% as.double()
      
      pred_idx &lt;-
        tf$multinomial(k_exp(preds), num_samples = 1)[1, 1] %&gt;% as.double()
      pred_word &lt;- index2word(pred_idx, target_index)
      
      if (pred_word == &#39;&lt;stop&gt;&#39;) {
        result &lt;-
          paste0(result, pred_word)
        return (list(result, sentence, attention_matrix))
      } else {
        result &lt;-
          paste0(result, pred_word, &quot; &quot;)
        dec_input &lt;- k_expand_dims(list(pred_idx))
      }
    }
    list(str_trim(result), sentence, attention_matrix)
  }

plot_attention &lt;-
  function(attention_matrix,
           words_sentence,
           words_result) {
    melted &lt;- melt(attention_matrix)
    ggplot(data = melted, aes(
      x = factor(Var2),
      y = factor(Var1),
      fill = value
    )) +
      geom_tile() + scale_fill_viridis() + guides(fill = FALSE) +
      theme(axis.ticks = element_blank()) +
      xlab(&quot;&quot;) +
      ylab(&quot;&quot;) +
      scale_x_discrete(labels = words_sentence, position = &quot;top&quot;) +
      scale_y_discrete(labels = words_result) + 
      theme(aspect.ratio = 1)
  }


translate &lt;- function(sentence) {
  c(result, sentence, attention_matrix) %&lt;-% evaluate(sentence)
  print(paste0(&quot;Input: &quot;,  sentence))
  print(paste0(&quot;Predicted translation: &quot;, result))
  attention_matrix &lt;-
    attention_matrix[1:length(str_split(result, &quot; &quot;)[[1]]),
                     1:length(str_split(sentence, &quot; &quot;)[[1]])]
  plot_attention(attention_matrix,
                 str_split(sentence, &quot; &quot;)[[1]],
                 str_split(result, &quot; &quot;)[[1]])
}</code></pre>
</div>
<h2 id="learning-to-translate">Learning to translate</h2>
<p>Using the <a href="https://github.com/rstudio/keras/tree/master/vignettes/examples/nmt_attention.R">sample code</a>, you can see yourself how learning progresses. This is how it worked in our case. (We are always looking at the same sentences - sampled from the training and test sets, respectively - so we can more easily see the evolution.)</p>
<p>On completion of the very first epoch, our network starts every Dutch sentence with <em>Ik</em>. No doubt, there must be many sentences starting in the first person in our corpus!</p>
<p>(Note: these five sentences are all from the training set.)</p>
<pre><code>
Input: &lt;start&gt; I did that easily . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik . &lt;stop&gt;

Input: &lt;start&gt; Look in the mirror . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik . &lt;stop&gt;

Input: &lt;start&gt; Tom wanted revenge . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik . &lt;stop&gt;

Input: &lt;start&gt; It s very kind of you . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik . &lt;stop&gt;

Input: &lt;start&gt; I refuse to answer . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik . &lt;stop&gt;</code></pre>
<p>One epoch later it seems to have picked up common phrases, although their use does not look related to the input. And definitely, it has problems to recognize when it’s over…</p>
<pre><code>
Input: &lt;start&gt; I did that easily . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik ben een een een een een een een een een een

Input: &lt;start&gt; Look in the mirror . &lt;stop&gt;
Predicted translation: &lt;start&gt; Tom is een een een een een een een een een een

Input: &lt;start&gt; Tom wanted revenge . &lt;stop&gt;
Predicted translation: &lt;start&gt; Tom is een een een een een een een een een een

Input: &lt;start&gt; It s very kind of you . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik ben een een een een een een een een een een

Input: &lt;start&gt; I refuse to answer . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik ben een een een een een een een een een een</code></pre>
<p>Jumping ahead to epoch 7, the translations still are completely wrong, but somehow start capturing overall sentence structure (like the imperative in sentence 2).</p>
<pre><code>
Input: &lt;start&gt; I did that easily . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik heb je niet . &lt;stop&gt;

Input: &lt;start&gt; Look in the mirror . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ga naar de buurt . &lt;stop&gt;

Input: &lt;start&gt; Tom wanted revenge . &lt;stop&gt;
Predicted translation: &lt;start&gt; Tom heeft Tom . &lt;stop&gt;

Input: &lt;start&gt; It s very kind of you . &lt;stop&gt;
Predicted translation: &lt;start&gt; Het is een auto . &lt;stop&gt;

Input: &lt;start&gt; I refuse to answer . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik heb de buurt . &lt;stop&gt;</code></pre>
<p>Fast forward to epoch 17. Samples from the training set are starting to look better:</p>
<pre><code>
Input: &lt;start&gt; I did that easily . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik heb dat hij gedaan . &lt;stop&gt;

Input: &lt;start&gt; Look in the mirror . &lt;stop&gt;
Predicted translation: &lt;start&gt; Kijk in de spiegel . &lt;stop&gt;

Input: &lt;start&gt; Tom wanted revenge . &lt;stop&gt;
Predicted translation: &lt;start&gt; Tom wilde dood . &lt;stop&gt;

Input: &lt;start&gt; It s very kind of you . &lt;stop&gt;
Predicted translation: &lt;start&gt; Het is erg goed voor je . &lt;stop&gt;

Input: &lt;start&gt; I refuse to answer . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik speel te antwoorden . &lt;stop&gt;</code></pre>
<p>Whereas samples from the test set still look pretty random. Although interestingly, not random in the sense of not having syntactic or semantic structure! <em>Breng de televisie op</em> is a perfectly reasonable sentence, if not the most lucky translation of <em>Think happy thoughts</em>.</p>
<pre><code>
Input: &lt;start&gt; It s entirely my fault . &lt;stop&gt;
Predicted translation: &lt;start&gt; Het is het mijn woord . &lt;stop&gt;

Input: &lt;start&gt; You re trustworthy . &lt;stop&gt;
Predicted translation: &lt;start&gt; Je bent net . &lt;stop&gt;

Input: &lt;start&gt; I want to live in Italy . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik wil in een leugen . &lt;stop&gt;

Input: &lt;start&gt; He has seven sons . &lt;stop&gt;
Predicted translation: &lt;start&gt; Hij heeft Frans uit . &lt;stop&gt;

Input: &lt;start&gt; Think happy thoughts . &lt;stop&gt;
Predicted translation: &lt;start&gt; Breng de televisie op . &lt;stop&gt;</code></pre>
<p>Where are we at after 30 epochs? By now, the training samples have been pretty much memorized (the third sentence is suffering from political correctness though, matching <em>Tom wanted revenge</em> to <em>Tom wilde vrienden</em>):</p>
<pre><code>
Input: &lt;start&gt; I did that easily . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik heb dat zonder moeite gedaan . &lt;stop&gt;

Input: &lt;start&gt; Look in the mirror . &lt;stop&gt;
Predicted translation: &lt;start&gt; Kijk in de spiegel . &lt;stop&gt;

Input: &lt;start&gt; Tom wanted revenge . &lt;stop&gt;
Predicted translation: &lt;start&gt; Tom wilde vrienden . &lt;stop&gt;

Input: &lt;start&gt; It s very kind of you . &lt;stop&gt;
Predicted translation: &lt;start&gt; Het is erg aardig van je . &lt;stop&gt;

Input: &lt;start&gt; I refuse to answer . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik weiger te antwoorden . &lt;stop&gt;</code></pre>
<p>How about the test sentences? They’ve started to look much better. One sentence (<em>Ik wil in Itali leven</em>) has even been translated entirely correctly. And we see something like the concept of numerals appearing (<em>seven</em> translated by <em>acht</em>)…</p>
<pre><code>
Input: &lt;start&gt; It s entirely my fault . &lt;stop&gt;
Predicted translation: &lt;start&gt; Het is bijna mijn beurt . &lt;stop&gt;

Input: &lt;start&gt; You re trustworthy . &lt;stop&gt;
Predicted translation: &lt;start&gt; Je bent zo zijn . &lt;stop&gt;

Input: &lt;start&gt; I want to live in Italy . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik wil in Itali leven . &lt;stop&gt;

Input: &lt;start&gt; He has seven sons . &lt;stop&gt;
Predicted translation: &lt;start&gt; Hij heeft acht geleden . &lt;stop&gt;

Input: &lt;start&gt; Think happy thoughts . &lt;stop&gt;
Predicted translation: &lt;start&gt; Zorg alstublieft goed uit . &lt;stop&gt;</code></pre>
<p>As you see it can be quite interesting watching the network’s “language capability” evolve. Now, how about subjecting our network to a little MRI scan? Since we’re collecting the attention weights, we can visualize what part of the source text the decoder is <em>attending to</em> at every timestep.</p>
<h2 id="what-is-the-decoder-looking-at">What is the decoder looking at?</h2>
<p>First, let’s take an example where word orders in both languages are the same.</p>
<pre><code>
Input: &lt;start&gt; It s very kind of you . &lt;stop&gt;
Predicted translation: &lt;start&gt; Het is erg aardig van je . &lt;stop&gt;</code></pre>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/train-4.png" width="500" /></p>
</div>
<p>We see that overall, given a sample where respective sentences align very well, the decoder pretty much looks where it is supposed to. Let’s pick something a little more complicated.</p>
<pre><code>
Input: &lt;start&gt; I did that easily . &lt;stop&gt;&quot;
Predicted translation: &lt;start&gt; Ik heb dat zonder moeite gedaan . &lt;stop&gt;</code></pre>
<p>The translation is correct, but word order in both languages isn’t the same here: <em>did</em> corresponds to the analytic perfect <em>heb … gedaan</em>. Will we be able to see that in the attention plot?</p>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/train-1.png" width="500" /></p>
</div>
<p>The answer is no. It would be interesting to check again after training for a couple more epochs.</p>
<p>Finally, let’s investigate this translation from the test set (which is entirely correct):</p>
<pre><code>
Input: &lt;start&gt; I want to live in Italy . &lt;stop&gt;
Predicted translation: &lt;start&gt; Ik wil in Itali leven . &lt;stop&gt;</code></pre>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/validation-3.png" width="500" /></p>
</div>
<p>These two sentences don’t align well. We see that Dutch <em>in</em> correctly picks English <em>in</em> (skipping over <em>to live</em>), then <em>Itali</em> attends to <em>Italy</em>. Finally <em>leven</em> is produced without us witnessing the decoder looking back to <em>live</em>. Here again, it would be interesting to watch what happens a few epochs later!</p>
<h2 id="next-up">Next up</h2>
<p>There are many ways to go from here. For one, we didn’t do any hyperparameter optimization. (See e.g. <span class="citation" data-cites="LuongPM15">(Luong, Pham, and Manning <a href="#ref-LuongPM15">2015</a>)</span> for an extensive experiment on architectures and hyperparameters for NMT.) Second, provided you have access to the required hardware, you might be curious how good an algorithm like this can get when trained on a real big dataset, using a real big network. Third, alternative attention mechanisms have been suggested (see e.g. <a href="https://github.com/lmthang/thesis/blob/master/thesis.pdf">T. Luong’s thesis</a> which we followed rather closely in the description of attention above).</p>
<p>Last not least, no one said attention need be useful only in the context of machine translation. Out there, a plenty of sequence prediction (time series) problems are waiting to be explored with respect to its potential usefulness…</p>
<div id="refs" class="references">
<div id="ref-BahdanauCB14">
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” <em>CoRR</em> abs/1409.0473. <a href="http://arxiv.org/abs/1409.0473" class="uri">http://arxiv.org/abs/1409.0473</a>.</p>
</div>
<div id="ref-LuongPM15">
<p>Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” <em>CoRR</em> abs/1508.04025. <a href="http://arxiv.org/abs/1508.04025" class="uri">http://arxiv.org/abs/1508.04025</a>.</p>
</div>
<div id="ref-VinyalsKKPSH14">
<p>Vinyals, Oriol, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. 2014. “Grammar as a Foreign Language.” <em>CoRR</em> abs/1412.7449. <a href="http://arxiv.org/abs/1412.7449" class="uri">http://arxiv.org/abs/1412.7449</a>.</p>
</div>
<div id="ref-XuBKCCSZB15">
<p>Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” <em>CoRR</em> abs/1502.03044. <a href="http://arxiv.org/abs/1502.03044" class="uri">http://arxiv.org/abs/1502.03044</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{LuongPM15,
  author    = {Minh{-}Thang Luong and
               Hieu Pham and
               Christopher D. Manning},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1508.04025},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.04025},
  archivePrefix = {arXiv},
  eprint    = {1508.04025},
  timestamp = {Wed, 07 Jun 2017 14:41:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LuongPM15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BritzGLL17,
  author    = {Denny Britz and
               Anna Goldie and
               Minh{-}Thang Luong and
               Quoc V. Le},
  title     = {Massive Exploration of Neural Machine Translation Architectures},
  journal   = {CoRR},
  volume    = {abs/1703.03906},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.03906},
  archivePrefix = {arXiv},
  eprint    = {1703.03906},
  timestamp = {Wed, 07 Jun 2017 14:41:30 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BritzGLL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VinyalsKKPSH14,
  author    = {Oriol Vinyals and
               Lukasz Kaiser and
               Terry Koo and
               Slav Petrov and
               Ilya Sutskever and
               Geoffrey E. Hinton},
  title     = {Grammar as a Foreign Language},
  journal   = {CoRR},
  volume    = {abs/1412.7449},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.7449},
  archivePrefix = {arXiv},
  eprint    = {1412.7449},
  timestamp = {Wed, 07 Jun 2017 14:40:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VinyalsKKPSH14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{XuBKCCSZB15,
  author    = {Kelvin Xu and
               Jimmy Ba and
               Ryan Kiros and
               Kyunghyun Cho and
               Aaron C. Courville and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Yoshua Bengio},
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual
               Attention},
  journal   = {CoRR},
  volume    = {abs/1502.03044},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03044},
  archivePrefix = {arXiv},
  eprint    = {1502.03044},
  timestamp = {Wed, 07 Jun 2017 14:43:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/XuBKCCSZB15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {CoRR},
  volume    = {abs/1409.0473},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0473},
  archivePrefix = {arXiv},
  eprint    = {1409.0473},
  timestamp = {Wed, 07 Jun 2017 14:40:19 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
